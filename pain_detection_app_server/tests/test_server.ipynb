{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5a9c510e",
      "metadata": {
        "id": "5a9c510e"
      },
      "source": [
        "## Step 1: Mount Google Drive and Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "08296257",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08296257",
        "outputId": "de5b20e0-e228-4117-a08d-25dd8db8377a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✓ Google Drive mounted\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "print(\"✓ Google Drive mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7227aa9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7227aa9b",
        "outputId": "d1b6d869-f385-4c74-ded0-8fb6dce6a572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'automatic-pain-recognition'...\n",
            "remote: Enumerating objects: 562, done.\u001b[K\n",
            "remote: Counting objects: 100% (285/285), done.\u001b[K\n",
            "remote: Compressing objects: 100% (227/227), done.\u001b[K\n",
            "remote: Total 562 (delta 115), reused 197 (delta 51), pack-reused 277 (from 1)\u001b[K\n",
            "Receiving objects: 100% (562/562), 23.63 MiB | 16.94 MiB/s, done.\n",
            "Resolving deltas: 100% (239/239), done.\n",
            "✓ Repository cloned. Current directory: /content/automatic-pain-recognition\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "!rm -rf automatic-pain-recognition\n",
        "!git clone https://github.com/alicka33/automatic-pain-recognition.git\n",
        "os.chdir('/content/automatic-pain-recognition')\n",
        "print(f\"✓ Repository cloned. Current directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f121b7fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f121b7fa",
        "outputId": "81a0740c-f87e-470e-8d3d-84bdc1d2f6a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Requirements installed\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "print(\"✓ Requirements installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f9ce0c6",
      "metadata": {
        "id": "4f9ce0c6"
      },
      "source": [
        "## Step 2: Add Project to Python Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JLsbNSkC1kUF",
      "metadata": {
        "id": "JLsbNSkC1kUF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"Cleaning and reinstalling MediaPipe for Colab...\")\n",
        "\n",
        "!pip uninstall -y mediapipe\n",
        "!rm -rf /usr/local/lib/python3.12/dist-packages/mediapipe\n",
        "!pip install --no-cache-dir mediapipe==0.10.14\n",
        "\n",
        "print(\"✓ MediaPipe 0.10.14 installed, restarting kernel...\")\n",
        "\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ad224243",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad224243",
        "outputId": "123725ba-b88f-45a5-a476-abdb3814a107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Working directory set to: /content/automatic-pain-recognition/pain_detection_app_server\n",
            "✓ Paths added to sys.path\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "project_root = Path('/content/automatic-pain-recognition')\n",
        "server_dir = project_root / 'pain_detection_app_server'\n",
        "\n",
        "for p in (project_root, server_dir):\n",
        "    if str(p) not in sys.path:\n",
        "        sys.path.insert(0, str(p))\n",
        "\n",
        "os.chdir(server_dir)\n",
        "print(f\"✓ Working directory set to: {os.getcwd()}\")\n",
        "print(f\"✓ Paths added to sys.path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93e2fc8",
      "metadata": {
        "id": "b93e2fc8"
      },
      "source": [
        "## Step 3: Test All Components\n",
        "\n",
        "Run each test below. ✓ = pass, ✗ = fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b66c0156",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b66c0156",
        "outputId": "e15d8085-48cd-4535-aede-e1049ecf0a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 1: Import Core Libraries\n",
            "======================================================================\n",
            "✓ PyTorch 2.9.0+cpu\n",
            "  CUDA available: False\n",
            "✓ NumPy 2.0.2\n",
            "✓ OpenCV 4.12.0\n",
            "✓ MediaPipe 0.10.14\n",
            "✓ FastAPI 0.123.10\n",
            "✓ scikit-learn 1.6.1\n",
            "\n",
            "Result: 6/6 library imports successful\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 1: Import Core Libraries\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tests_passed = 0\n",
        "tests_total = 0\n",
        "\n",
        "# PyTorch\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"✓ PyTorch {torch.__version__}\")\n",
        "    print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    tests_passed += 1\n",
        "except ImportError as e:\n",
        "    print(f\"✗ PyTorch: {e}\")\n",
        "tests_total += 1\n",
        "\n",
        "# NumPy\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(f\"✓ NumPy {np.__version__}\")\n",
        "    tests_passed += 1\n",
        "except ImportError as e:\n",
        "    print(f\"✗ NumPy: {e}\")\n",
        "tests_total += 1\n",
        "\n",
        "# OpenCV\n",
        "try:\n",
        "    import cv2\n",
        "    print(f\"✓ OpenCV {cv2.__version__}\")\n",
        "    tests_passed += 1\n",
        "except ImportError as e:\n",
        "    print(f\"✗ OpenCV: {e}\")\n",
        "tests_total += 1\n",
        "\n",
        "# MediaPipe\n",
        "try:\n",
        "    import mediapipe as mp\n",
        "    print(f\"✓ MediaPipe {mp.__version__}\")\n",
        "    tests_passed += 1\n",
        "except ImportError as e:\n",
        "    print(f\"✗ MediaPipe: {e}\")\n",
        "tests_total += 1\n",
        "\n",
        "# FastAPI\n",
        "try:\n",
        "    import fastapi\n",
        "    print(f\"✓ FastAPI {fastapi.__version__}\")\n",
        "    tests_passed += 1\n",
        "except ImportError as e:\n",
        "    print(f\"✗ FastAPI: {e}\")\n",
        "tests_total += 1\n",
        "\n",
        "# scikit-learn\n",
        "try:\n",
        "    import sklearn\n",
        "    print(f\"✓ scikit-learn {sklearn.__version__}\")\n",
        "    tests_passed += 1\n",
        "except ImportError as e:\n",
        "    print(f\"✗ scikit-learn: {e}\")\n",
        "tests_total += 1\n",
        "\n",
        "print(f\"\\nResult: {tests_passed}/{tests_total} library imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b9957011",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9957011",
        "outputId": "45fe60bf-59f1-4244-d7b3-627c8081cd7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 2: Configuration File\n",
            "======================================================================\n",
            "✓ Config loaded successfully\n",
            "\n",
            "  Model Configuration:\n",
            "    Model Type: sta_lstm\n",
            "    Num Classes: 2\n",
            "    Num Features: 300\n",
            "    Hidden Size: 128\n",
            "    Max Sequence Length: 200\n",
            "\n",
            "  Path Configuration:\n",
            "    Model Checkpoint: /content/automatic-pain-recognition/pain_detection_app_server/model/model_paths/testing_new_code_sta_lstm_2_classes_300_coord.pt\n",
            "    Data Dir: /content/automatic-pain-recognition/pain_detection_app_server/data\n",
            "\n",
            "Result: Config test PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 2: Configuration File\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    import config\n",
        "    print(f\"✓ Config loaded successfully\")\n",
        "    print(f\"\\n  Model Configuration:\")\n",
        "    print(f\"    Model Type: {config.MODEL_TYPE}\")\n",
        "    print(f\"    Num Classes: {config.NUM_CLASSES}\")\n",
        "    print(f\"    Num Features: {config.NUM_FEATURES}\")\n",
        "    print(f\"    Hidden Size: {config.HIDDEN_SIZE}\")\n",
        "    print(f\"    Max Sequence Length: {config.MAX_SEQUENCE_LENGTH}\")\n",
        "    print(f\"\\n  Path Configuration:\")\n",
        "    print(f\"    Model Checkpoint: {config.MODEL_CHECKPOINT_PATH}\")\n",
        "    print(f\"    Data Dir: {config.DATA_DIR}\")\n",
        "    tests_passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"✗ Config error: {e}\")\n",
        "tests_total += 1\n",
        "\n",
        "print(f\"\\nResult: Config test {'PASSED' if tests_passed == tests_total else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70807c1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70807c1a",
        "outputId": "aad526fd-bdff-42ea-beb2-768ade1bf850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 3: Model File Existence\n",
            "======================================================================\n",
            "✓ Model checkpoint found\n",
            "  Path: /content/automatic-pain-recognition/pain_detection_app_server/model/model_paths/testing_new_code_sta_lstm_2_classes_300_coord.pt\n",
            "  Size: 3.62 MB\n",
            "\n",
            "Optional Files:\n",
            "  ✓ Reference keypoints found\n",
            "  ✓ Landmark indices found\n",
            "\n",
            "Result: Model file test PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 3: Model File Existence\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import config\n",
        "from pathlib import Path\n",
        "\n",
        "tests_total += 1\n",
        "\n",
        "if config.MODEL_CHECKPOINT_PATH.exists():\n",
        "    size_mb = config.MODEL_CHECKPOINT_PATH.stat().st_size / (1024 * 1024)\n",
        "    print(f\"✓ Model checkpoint found\")\n",
        "    print(f\"  Path: {config.MODEL_CHECKPOINT_PATH}\")\n",
        "    print(f\"  Size: {size_mb:.2f} MB\")\n",
        "    tests_passed += 1\n",
        "else:\n",
        "    print(f\"✗ Model checkpoint NOT found\")\n",
        "    print(f\"  Expected at: {config.MODEL_CHECKPOINT_PATH}\")\n",
        "    print(f\"  Add your trained model file there\")\n",
        "\n",
        "print(f\"\\nOptional Files:\")\n",
        "if config.REFERENCE_KEYPOINTS_PATH.exists():\n",
        "    print(f\"  ✓ Reference keypoints found\")\n",
        "else:\n",
        "    print(f\"  ✗ Reference keypoints not found (frontalization disabled)\")\n",
        "\n",
        "if config.LANDMARK_INDICES_PATH.exists():\n",
        "    print(f\"  ✓ Landmark indices found\")\n",
        "else:\n",
        "    print(f\"  ✗ Landmark indices not found (using all landmarks)\")\n",
        "\n",
        "print(f\"\\nResult: Model file test {'PASSED' if tests_passed == tests_total else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bed8e971",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bed8e971",
        "outputId": "14d44f20-aad0-457a-ad7f-d673cc8390f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 4: Video Processing Pipeline\n",
            "======================================================================\n",
            "✓ MediaPipe pipeline imports successful\n",
            "✓ FaceMesh instance created\n",
            "✓ FaceMesh closed properly\n",
            "\n",
            "Result: Video processing test PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 4: Video Processing Pipeline\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tests_total += 1\n",
        "try:\n",
        "    from services.processing_pipeline_mediapipe import (\n",
        "        create_face_mesh,\n",
        "        parse_landmarks_from_results\n",
        "    )\n",
        "    print(\"✓ MediaPipe pipeline imports successful\")\n",
        "\n",
        "    face_mesh = create_face_mesh()\n",
        "    print(\"✓ FaceMesh instance created\")\n",
        "    face_mesh.close()\n",
        "    print(\"✓ FaceMesh closed properly\")\n",
        "    tests_passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"✗ Video processing error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\nResult: Video processing test {'PASSED' if tests_passed == tests_total else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3bde83f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bde83f8",
        "outputId": "7fb780db-06f9-45f1-93a1-ee76eb2822d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 5: Inference Module Imports\n",
            "======================================================================\n",
            "✓ VideoInferenceHelper import successful\n",
            "✓ load_model_for_inference import successful\n",
            "\n",
            "Result: Inference module test PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 5: Inference Module Imports\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tests_total += 1\n",
        "try:\n",
        "    from services.video_inference import VideoInferenceHelper, load_model_for_inference\n",
        "    print(\"✓ VideoInferenceHelper import successful\")\n",
        "    print(\"✓ load_model_for_inference import successful\")\n",
        "    tests_passed += 1\n",
        "except Exception as e:\n",
        "    print(f\"✗ Inference module error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\nResult: Inference module test {'PASSED' if tests_passed == tests_total else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4d16e793",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d16e793",
        "outputId": "3f956411-f2ae-492d-811e-5ed61bbf35a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 6: Model Loading\n",
            "======================================================================\n",
            "  Loading model on device: cpu\n",
            "✓ Model loaded successfully\n",
            "  Model Type: sta_lstm\n",
            "  Device: cpu\n",
            "  Parameters: 946,863\n",
            "\n",
            "Result: Model loading test PASSED\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 6: Model Loading\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import config\n",
        "\n",
        "tests_total += 1\n",
        "\n",
        "if not config.MODEL_CHECKPOINT_PATH.exists():\n",
        "    print(f\"✗ Skipping (no model checkpoint found)\")\n",
        "    print(f\"  Add your trained model to: {config.MODEL_CHECKPOINT_PATH}\")\n",
        "else:\n",
        "    try:\n",
        "        import torch\n",
        "        from services.video_inference import load_model_for_inference\n",
        "\n",
        "        if config.MODEL_TYPE == \"attention_lstm\":\n",
        "            from model.Attention_LSTM import AttentionSequenceModel as ModelClass\n",
        "        elif config.MODEL_TYPE == \"bi_lstm\":\n",
        "            from model.Bi_LSTM import SequenceModel as ModelClass\n",
        "        elif config.MODEL_TYPE == \"sta_lstm\":\n",
        "            from model.STA_LSTM import STA_LSTM as ModelClass\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {config.MODEL_TYPE}\")\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"  Loading model on device: {device}\")\n",
        "\n",
        "        model = load_model_for_inference(\n",
        "            model_class=ModelClass,\n",
        "            model_path=str(config.MODEL_CHECKPOINT_PATH),\n",
        "            device=device,\n",
        "            input_size=config.NUM_FEATURES,\n",
        "            hidden_size=config.HIDDEN_SIZE,\n",
        "            num_layers=config.NUM_LAYERS,\n",
        "            num_classes=config.NUM_CLASSES,\n",
        "            dropout_prob=config.DROPOUT_PROB\n",
        "        )\n",
        "\n",
        "        num_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"✓ Model loaded successfully\")\n",
        "        print(f\"  Model Type: {config.MODEL_TYPE}\")\n",
        "        print(f\"  Device: {device}\")\n",
        "        print(f\"  Parameters: {num_params:,}\")\n",
        "        tests_passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Model loading error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(f\"\\nResult: Model loading test {'PASSED' if tests_passed == tests_total else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7558148e",
      "metadata": {
        "id": "7558148e"
      },
      "source": [
        "## Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d4ed8a3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4ed8a3a",
        "outputId": "d64cccf8-4938-4587-f9a4-d209fb02b2be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "FINAL TEST SUMMARY\n",
            "======================================================================\n",
            "✓ Core Libraries                           PASS\n",
            "✓ Configuration                            PASS\n",
            "✓ Video Processing                         PASS\n",
            "✓ Inference Module                         PASS\n",
            "✓ Model Checkpoint Present                 PASS\n",
            "\n",
            "======================================================================\n",
            "All checks passed (model found). Server ready to run.\n",
            "Next: run `python app.py` or `uvicorn app:app --reload --port 7860`\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL TEST SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import config\n",
        "model_present = config.MODEL_CHECKPOINT_PATH.exists()\n",
        "summary = {\n",
        "    \"Core Libraries\": True,\n",
        "    \"Configuration\": True,\n",
        "    \"Video Processing\": True,\n",
        "    \"Inference Module\": True,\n",
        "    \"Model Checkpoint Present\": model_present,\n",
        "}\n",
        "\n",
        "for test_name, passed in summary.items():\n",
        "    status = \"PASS\" if passed else \"MISSING\"\n",
        "    symbol = \"✓\" if passed else \"✗\"\n",
        "    print(f\"{symbol} {test_name:40} {status}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "if model_present:\n",
        "    print(\"All checks passed (model found). Server ready to run.\")\n",
        "    print(\"Next: run `python app.py` or `uvicorn app:app --reload --port 7860`\")\n",
        "else:\n",
        "    print(\"Model checkpoint missing. Add your trained model and re-run Test 6.\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
